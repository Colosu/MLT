{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Transformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Lj-YUmK9pzOF","executionInfo":{"status":"ok","timestamp":1638487110025,"user_tz":-60,"elapsed":3057,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["import numpy as np\n","import pandas as pd\n","import csv\n","import subprocess\n","from pandas import DataFrame\n","from matplotlib import pyplot\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_rUUDGUF8L1","executionInfo":{"status":"ok","timestamp":1638487110026,"user_tz":-60,"elapsed":15,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["#Transformer vars\n","vocab_size = 2556  # Only consider the top 20k words\n","maxlen = 20  # Max sequence size\n","jump = 1 # Jump between inputs\n","embed_dim = 256  # Embedding size for each token\n","num_att_layers = 1 # Number of attention layers\n","num_heads = 12  # Number of attention heads\n","feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n","batch_size = 256\n","top_k = 10\n","dropout = 0.1\n","training_epochs = 100"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAorajiyd09r","executionInfo":{"status":"ok","timestamp":1638487110027,"user_tz":-60,"elapsed":15,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def to_row(trace):\n","    strings = []\n","    string = \"\"\n","    space = True\n","    jump = False\n","    for elem in trace:\n","      if elem == \"<\":\n","        jump = True\n","      if jump == False:\n","        if elem == \" \":\n","          space = True\n","          strings.append(string)\n","          string = \"\"\n","        else:\n","          if space:\n","            string = elem\n","          else:\n","            string += \"/\" + elem\n","          space = False\n","      elif elem == \" \":\n","        jump = False\n","    return strings"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiqIr8G0qPdI","executionInfo":{"status":"ok","timestamp":1638487110028,"user_tz":-60,"elapsed":15,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def print_tensor_traces(trace_tensor, output_file, headers):\n","    with open(output_file, 'w') as f:\n","      w = csv.writer(f, delimiter=';')\n","      w.writerow(headers)\n","      for trace in trace_tensor:\n","        strings = to_row(trace)\n","        w.writerow(strings)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQQzkzBGFvAm","executionInfo":{"status":"ok","timestamp":1638487110028,"user_tz":-60,"elapsed":14,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n","    \"\"\"\n","    Mask the upper half of the dot product matrix in self attention.\n","    This prevents flow of information from future tokens to current token.\n","    1's in the lower triangle, counting from the lower right corner.\n","    \"\"\"\n","    i = tf.range(n_dest)[:, None]\n","    j = tf.range(n_src)\n","    m = i >= j - n_src + n_dest\n","    mask = tf.cast(m, dtype)\n","    mask = tf.reshape(mask, [1, n_dest, n_src])\n","    mult = tf.concat(\n","        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n","    )\n","    return tf.tile(mask, mult)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-AcCY8-FxR9","executionInfo":{"status":"ok","timestamp":1638487110029,"user_tz":-60,"elapsed":14,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.mask_att = layers.MultiHeadAttention(num_heads, embed_dim)\n","        self.att1 = layers.MultiHeadAttention(num_heads, embed_dim)\n","        self.att2 = layers.MultiHeadAttention(num_heads, embed_dim)\n","        self.ffn1 = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.ffn2 = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm4 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm5 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","        self.dropout3 = layers.Dropout(rate)\n","        self.dropout4 = layers.Dropout(rate)\n","        self.dropout5 = layers.Dropout(rate)\n","        \n","    def call(self, inputs, outputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size = input_shape[0]\n","        seq_len = input_shape[1]\n","        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        # Left block\n","        attention1_output = self.att1(inputs, inputs, attention_mask=causal_mask)\n","        attention1_output = self.dropout1(attention1_output)\n","        out1 = self.layernorm1(inputs + attention1_output)\n","        ffn1_output = self.ffn1(out1)\n","        ffn1_output = self.dropout2(ffn1_output)\n","        out2 = self.layernorm2(out1 + ffn1_output)\n","\n","        # Right block\n","        attention_mask_output = self.mask_att(outputs, outputs, attention_mask=causal_mask)\n","        attention_mask_output = self.dropout3(attention_mask_output)\n","        out3 = self.layernorm3(outputs + attention_mask_output)\n","        #causal_mask2 = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        attention2_output = self.att2(out3, out2, attention_mask=causal_mask)\n","        attention2_output = self.dropout4(attention2_output)\n","        out4 = self.layernorm4(out3 + attention2_output)\n","        ffn2_output = self.ffn2(out4)\n","        ffn2_output = self.dropout5(ffn2_output)\n","        return self.layernorm5(out4 + ffn2_output)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gvnt6oRzF4b-","executionInfo":{"status":"ok","timestamp":1638487110030,"user_tz":-60,"elapsed":14,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGtQyuHeGBAu","executionInfo":{"status":"ok","timestamp":1638487110030,"user_tz":-60,"elapsed":13,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def create_model():\n","    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n","    embedding_layer1 = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","    embedding_layer2 = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","    x = embedding_layer1(inputs)\n","    y = embedding_layer2(inputs)\n","    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim, rate=dropout)\n","    z = transformer_block(x, y)\n","    outputs = layers.Dense(vocab_size)(z)\n","    model = keras.Model(inputs=inputs, outputs=[outputs, z])\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    model.compile(\n","        \"adam\", loss=[loss_fn, None],\n","    )  # No loss and optimization based on word embeddings from transformer block\n","    return model"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNOx9Y6v2O7p","executionInfo":{"status":"ok","timestamp":1638487110031,"user_tz":-60,"elapsed":14,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def custom_standardization(input_string):\n","    return tf.strings.regex_replace(input_string, f\"([/])\", r\"\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"C10Lt61a4nK8","executionInfo":{"status":"ok","timestamp":1638487110031,"user_tz":-60,"elapsed":13,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def add_pads(traces):\n","  addition = []\n","  for r in traces:\n","    if r.split(\" \")[0] == \"<init>\":\n","      string = \"\"\n","      leng = 0\n","      for elem in r.split(\" \"):\n","        if elem == \"<init>\":\n","          string += elem\n","        else:\n","          string += \" \" + elem\n","        leng += 1\n","        pads = \"\".join([\" <PAD>\"]*(maxlen - leng))\n","        new = string + pads\n","        addition.append(new)\n","  return np.append(traces, addition)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOm9SL7eA0t-","executionInfo":{"status":"ok","timestamp":1638487110032,"user_tz":-60,"elapsed":14,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def crop_pads(traces, size):\n","  new_traces = []\n","  for r in traces:\n","    string = []\n","    leng = 0\n","    for elem in r.split(\" \"):\n","      if elem == \"<init>\":\n","        string.append(elem)\n","      else:\n","        string.append(\" \" + elem)\n","      leng += 1\n","      if leng >= size and (leng%jump == 0 or elem == \"<end>\"):\n","        new = string[-size:]\n","        new = \"\".join(new)\n","        new_traces.append(new)\n","  return new_traces"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxzYhqtLVT28","executionInfo":{"status":"ok","timestamp":1638487112642,"user_tz":-60,"elapsed":2623,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}},"outputId":"46b9caca-5d53-48ee-c45b-33ce4e325848"},"source":["file_num = 1\n","source_file = \"fsm\"+str(file_num)+\" Dataset\"\n","data = pd.read_csv(\"./drive/MyDrive/Colab Notebooks/data/\" + source_file + \".csv\", sep=\";\", keep_default_na=False)\n","\n","y = data.iloc[:,-1:]\n","data = data.drop(data.columns[-1],axis=1)\n","headers = data.columns\n","traces = data.copy()\n","traces.insert(0, \"init\", [\"<init>\"]*traces.shape[0], True)\n","traces['end'] = [\"<end>\"]*traces.shape[0]\n","print(traces.head())\n","traces = traces.agg(\" \".join, axis=1)\n","traces = traces.astype('string')\n","traces = traces.to_numpy()\n","traces = crop_pads(traces,maxlen)\n","#traces = add_pads(traces)\n","print(traces[0:5])\n","print(traces[-5:])\n","print(len(traces))\n","\n","text_ds = tf.data.Dataset.from_tensor_slices(traces)\n","text_ds = text_ds.shuffle(buffer_size=len(traces))\n","text_ds = text_ds.batch(batch_size)\n","\n","# Create a vectorization layer and adapt it to the text\n","vectorize_layer = TextVectorization(standardize=custom_standardization, max_tokens=vocab_size - 1, output_mode=\"int\", output_sequence_length=maxlen + 1)\n","vectorize_layer.adapt(text_ds)\n","vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n","vocab.append(\"<PAD>\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["     init    1    2    3    4    5    6  ...   95   96   97   98   99  100    end\n","0  <init>  U/M  a/T  K/B  S/U  k/C  M/\\  ...  p/D  o/r  ^/j  U/F  A/p  [/G  <end>\n","1  <init>  R/f  M/]  n/_  a/L  h/c  k/n  ...  J/L  a/\\  ^/[  X/_  a/_  h/K  <end>\n","2  <init>  f/\\  Y/J  p/R  U/B  `/H  f/r  ...  n/L  Z/E  ]/Q  L/R  Q/]  n/Z  <end>\n","3  <init>  i/[  F/e  c/Q  B/A  e/j  a/k  ...  V/]  p/S  W/Z  O/F  V/T  a/^  <end>\n","4  <init>  `/b  c/n  J/M  g/B  X/p  Z/a  ...  I/E  m/T  o/c  W/q  n/O  A/p  <end>\n","\n","[5 rows x 102 columns]\n","['<init> U/M a/T K/B S/U k/C M/\\\\ P/q q/C R/Z U/M c/Y a/[ f/_ [/H Q/k ]/o o/H n/R `/h', ' U/M a/T K/B S/U k/C M/\\\\ P/q q/C R/Z U/M c/Y a/[ f/_ [/H Q/k ]/o o/H n/R `/h K/P', ' a/T K/B S/U k/C M/\\\\ P/q q/C R/Z U/M c/Y a/[ f/_ [/H Q/k ]/o o/H n/R `/h K/P o/B', ' K/B S/U k/C M/\\\\ P/q q/C R/Z U/M c/Y a/[ f/_ [/H Q/k ]/o o/H n/R `/h K/P o/B C/O', ' S/U k/C M/\\\\ P/q q/C R/Z U/M c/Y a/[ f/_ [/H Q/k ]/o o/H n/R `/h K/P o/B C/O A/q']\n","[' F/V F/Q C/L O/D A/F P/\\\\ m/X Z/r K/T J/R ]/k Q/Y X/O j/c n/R J/C o/F L/L Z/N h/K', ' F/Q C/L O/D A/F P/\\\\ m/X Z/r K/T J/R ]/k Q/Y X/O j/c n/R J/C o/F L/L Z/N h/K M/_', ' C/L O/D A/F P/\\\\ m/X Z/r K/T J/R ]/k Q/Y X/O j/c n/R J/C o/F L/L Z/N h/K M/_ G/A', ' O/D A/F P/\\\\ m/X Z/r K/T J/R ]/k Q/Y X/O j/c n/R J/C o/F L/L Z/N h/K M/_ G/A h/[', ' A/F P/\\\\ m/X Z/r K/T J/R ]/k Q/Y X/O j/c n/R J/C o/F L/L Z/N h/K M/_ G/A h/[ <end>']\n","83000\n"]}]},{"cell_type":"code","metadata":{"id":"3Ddju6JMGgO1","executionInfo":{"status":"ok","timestamp":1638487112643,"user_tz":-60,"elapsed":9,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["def prepare_lm_inputs_labels(text):\n","    \"\"\"\n","    Shift word sequences by 1 position so that the target for position (i) is\n","    word at position (i+1). The model will use all words up till position (i)\n","    to predict the next word.\n","    \"\"\"\n","    text = tf.expand_dims(text, -1)\n","    tokenized_sentences = vectorize_layer(text)\n","    x = tokenized_sentences[:, :-1]\n","    y = tokenized_sentences[:, 1:]\n","    return x, y"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWyo-szwGhfV","executionInfo":{"status":"ok","timestamp":1638487112644,"user_tz":-60,"elapsed":9,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["text_ds = text_ds.map(prepare_lm_inputs_labels)\n","text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zxziw0L1Gk2F","executionInfo":{"status":"ok","timestamp":1638487112644,"user_tz":-60,"elapsed":8,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["class TextGenerator(keras.callbacks.Callback):\n","    \"\"\"A callback to generate text from a trained model.\n","    1. Feed some starting prompt to the model\n","    2. Predict probabilities for the next token\n","    3. Sample the next token and add it to the next input\n","\n","    Arguments:\n","        max_tokens: Integer, the number of tokens to be generated after prompt.\n","        start_tokens: List of integers, the token indices for the starting prompt.\n","        index_to_word: List of strings, obtained from the TextVectorization layer.\n","        top_k: Integer, sample from the `top_k` token predictions.\n","        print_every: Integer, print after this many epochs.\n","    \"\"\"\n","\n","    def __init__(\n","        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n","    ):\n","        self.max_tokens = max_tokens\n","        self.start_tokens = start_tokens\n","        self.index_to_word = index_to_word\n","        self.print_every = print_every\n","        self.k = top_k\n","\n","    def sample_from(self, logits):\n","        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n","        indices = np.asarray(indices).astype(\"int32\")\n","        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n","        preds = np.asarray(preds).astype(\"float32\")\n","        return np.random.choice(indices, p=preds)\n","\n","    def detokenize(self, number):\n","        return self.index_to_word[number]\n","    \n","    def on_train_begin(self,logs={}):\n","        self.losses = []\n","        self.accuracies = []\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        self.losses.append(logs.get('loss'))\n","        start_tokens = [_ for _ in self.start_tokens]\n","        if (epoch + 1) % self.print_every != 0:\n","            return\n","        num_tokens_generated = 0\n","        tokens_generated = []\n","        last_token = 1\n","        while num_tokens_generated <= self.max_tokens and last_token != vocab.index(\"<end>\"):\n","            pad_len = maxlen - len(start_tokens)\n","            sample_index = len(start_tokens) - 1\n","            if pad_len <= 0:\n","                x = start_tokens[-(maxlen-1):] + [vocab.index(\"<PAD>\")]\n","                sample_index = maxlen - 2\n","            elif pad_len > 0:\n","                x = start_tokens + [vocab.index(\"<PAD>\")] * pad_len\n","            x = np.array([x])\n","            y, _ = self.model.predict(x)\n","            sample_token = self.sample_from(y[0][sample_index])\n","            tokens_generated.append(sample_token)\n","            start_tokens.append(sample_token)\n","            num_tokens_generated = len(tokens_generated)\n","            last_token = sample_token\n","        txt = \" \".join(\n","            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n","        )\n","        print(f\"generated text:\\n{txt}\\n\")\n","        plt.plot(self.losses)\n","        plt.title('Loss')\n","        plt.show()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"F87ScclNGo5N","executionInfo":{"status":"ok","timestamp":1638487112645,"user_tz":-60,"elapsed":8,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["# Tokenize starting prompt\n","word_to_index = {}\n","for index, word in enumerate(vocab):\n","    word_to_index[word] = index\n","\n","start_prompt = \"<init>\"\n","start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n","num_tokens_generated = 100\n","text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab, top_k=top_k)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnq_bxhPGrUN","outputId":"e63b98dc-75d0-4082-fd41-2c7f625d5c9e"},"source":["model = create_model()\n","model.fit(text_ds, verbose=2, epochs=training_epochs, callbacks=[text_gen_callback])\n","model.save(\"trasformer\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]}]},{"cell_type":"code","metadata":{"id":"9s1zCPnNXz0d","executionInfo":{"status":"ok","timestamp":1638488312421,"user_tz":-60,"elapsed":7,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["class TextGen():\n","    \"\"\"A callback to generate text from a trained model.\n","    1. Feed some starting prompt to the model\n","    2. Predict probabilities for the next token\n","    3. Sample the next token and add it to the next input\n","\n","    Arguments:\n","        max_tokens: Integer, the number of tokens to be generated after prompt.\n","        start_tokens: List of integers, the token indices for the starting prompt.\n","        index_to_word: List of strings, obtained from the TextVectorization layer.\n","        top_k: Integer, sample from the `top_k` token predictions.\n","        print_every: Integer, print after this many epochs.\n","    \"\"\"\n","\n","    def __init__(self, model, max_tokens, start_tokens, index_to_word, top_k=10):\n","        self.model = model\n","        self.max_tokens = max_tokens\n","        self.start_tokens = start_tokens\n","        self.index_to_word = index_to_word\n","        self.k = top_k\n","\n","    def sample_from(self, logits):\n","        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n","        indices = np.asarray(indices).astype(\"int32\")\n","        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n","        preds = np.asarray(preds).astype(\"float32\")\n","        return np.random.choice(indices, p=preds)\n","\n","    def detokenize(self, number):\n","        return self.index_to_word[number]\n","\n","    def gen_traces(self, num_traces, logs=None):\n","        traces = []\n","        for epoch in range(num_traces):\n","            start_tokens = [_ for _ in self.start_tokens]\n","            num_tokens_generated = 0\n","            tokens_generated = []\n","            last_token = 1\n","            while num_tokens_generated <= self.max_tokens and last_token != vocab.index(\"<end>\"):\n","                pad_len = maxlen - len(start_tokens)\n","                sample_index = len(start_tokens) - 1\n","                if pad_len <= 0:\n","                    x = start_tokens[-(maxlen-1):] + [vocab.index(\"<PAD>\")]\n","                    sample_index = maxlen - 2\n","                elif pad_len > 0:\n","                    x = start_tokens + [vocab.index(\"<PAD>\")] * pad_len\n","                x = np.array([x])\n","                y, _ = self.model.predict(x)\n","                sample_token = self.sample_from(y[0][sample_index])\n","                tokens_generated.append(sample_token)\n","                start_tokens.append(sample_token)\n","                num_tokens_generated = len(tokens_generated)\n","                last_token = sample_token\n","            txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n","            #print(f\"generated text:\\n{txt}\\n\")\n","            print(f\"trace {epoch}\")\n","            traces.append(txt)\n","        return traces"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5zdpDV6-lnq"},"source":["# Tokenize starting prompt\n","word_to_index = {}\n","for index, word in enumerate(vocab):\n","    word_to_index[word] = index\n","\n","start_prompt = \"<init>\"\n","start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n","length = 100\n","num_traces = 100\n","model = keras.models.load_model(\"trasformer\")\n","text_generator = TextGen(model, length, start_tokens, vocab, top_k=top_k)\n","generated_traces = text_generator.gen_traces(num_traces)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPEnGux3_Jjo"},"source":["print_tensor_traces(generated_traces, \"./drive/MyDrive/Colab Notebooks/results/traces_\" + source_file + \".csv\" , headers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m38_lHLOiwIR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638488701758,"user_tz":-60,"elapsed":7365,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}},"outputId":"30fadfe7-5301-4394-b4d8-755b7006393e"},"source":["#Update and upgrade the system before installing anything else.\n","!apt-get update > /dev/null\n","!apt-get upgrade > /dev/null\n","\n","#Install the Java JDK.\n","!apt-get install default-jdk > /dev/null\n","\n","#Check the Java version to see if everything is working well.\n","!javac -version"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["javac 11.0.11\n"]}]},{"cell_type":"code","metadata":{"id":"JGQxD4TImLP_","executionInfo":{"status":"ok","timestamp":1638488701759,"user_tz":-60,"elapsed":5,"user":{"displayName":"Alfredo Ibias Martínez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikLu1-K9CqFtnaizrxAVLdP_qUXXgJac-mAdMO=s64","userId":"09843974926286631870"}}},"source":["#!java -jar \"./drive/MyDrive/Colab Notebooks/CheckTraces.jar\""],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxtvn759Rf1X"},"source":["valid = subprocess.run(['java', '-jar', './drive/MyDrive/Colab Notebooks/CheckTraces.jar', str(file_num)], stdout=subprocess.PIPE).stdout.decode('utf-8')\n","valid = float(valid.split(' ')[3][:-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwSfTxEs7CeM"},"source":["count = 0\n","total = 0\n","for t in generated_traces:\n","  t = to_row(t)\n","  for tt in np.array(data):\n","    if t[0] == tt[0]:\n","      same = True\n","      for elem, elem2 in zip(t, tt):\n","          if elem != elem2:\n","            same = False\n","      if same:\n","        count += 1\n","  total += 1\n","subsets = count/total\n","count = 0\n","total = 0\n","for t in generated_traces:\n","  t = to_row(t)\n","  for tt in np.array(data):\n","    if t[0] == tt[0] and len(t) == len(tt):\n","      same = True\n","      for elem, elem2 in zip(t, tt):\n","          if elem != elem2:\n","            same = False\n","      if same:\n","        count += 1\n","  total += 1\n","intersect = count/total\n","count = 0\n","total = 0\n","reps = []\n","for i in range(len(generated_traces)):\n","  t = to_row(generated_traces[i])\n","  same = False\n","  for j in range(len(generated_traces) - i - 1):\n","    tt = to_row(generated_traces[j+i+1])\n","    if t[0] == tt[0] and len(t) == len(tt) and not same:\n","      same = True\n","      for elem, elem2 in zip(t, tt):\n","          if elem != elem2:\n","            same = False\n","      if same:\n","        count += 1\n","        reps.append(t)\n","  total += 1\n","repeated = count/total\n","count = 0\n","total = 0\n","for t in reps:\n","  same = False\n","  for tt in np.array(data):\n","    if t[0] == tt[0] and len(t) == len(tt) and not same:\n","      same = True\n","      for elem, elem2 in zip(t, tt):\n","          if elem != elem2:\n","            same = False\n","      if same:\n","        count += 1\n","  total += 1\n","if total > 0:\n","  old_repeated = count/total\n","else:\n","  old_repeated = 0\n","print(f\"Valid traces: {valid:0.3f}%\")\n","print()\n","print(f\"Traces seen from dataset: {subsets:0.3f}%\")\n","print(f\"Traces copied from dataset: {intersect:0.3f}%\")\n","subs = subsets-intersect\n","print(f\"Traces subset of traces from dataset: {subs:0.3f}%\")\n","print()\n","new = 1 - subsets\n","print(f\"New unseen traces: {new:0.3f}%\")\n","unseen = valid - subsets\n","print(f\"New unseen and valid traces: {unseen:0.3f}%\")\n","nonvalid = 1 - valid\n","print(f\"New unseen and non-valid traces: {nonvalid:0.3f}%\")\n","print()\n","print(f\"Traces repeated: {repeated:0.3f}%\")\n","print(f\"Traces repeated and in dataset: {old_repeated:0.3f}%\")\n","new_repeated = 1 - old_repeated\n","print(f\"New unseen and repeated traces: {new_repeated:0.3f}%\")"],"execution_count":null,"outputs":[]}]}